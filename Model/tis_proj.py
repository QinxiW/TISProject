# -*- coding: utf-8 -*-
"""TIS_proj.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i1V6xnXwEMrUZTKT0O6cvsC8tBST0mS9
"""

from google.colab import drive
drive.mount('/content/drive')

import torch
import random
import numpy as np

seed = 42
np.random.seed(seed)
random.seed(seed)
torch.manual_seed(seed)
torch.cuda.manual_seed(seed)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False
print(f"Random seed set as {seed}")

torch.cuda.empty_cache()

MAIN_DIR = "drive/MyDrive"
# MAIN_DIR = "drive/MyDrive/metamia"

# !pip install transformers
!pip install transformers[torch]

import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset, random_split
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
import json
import pandas as pd

df = pd.read_csv(f"{MAIN_DIR}/cleaned_data.csv.gz")
df.head()

"""# New Section"""

df.columns

df.dtypes

# train_texts, train_labels = df['document'].values, df['label'].values

# tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
# model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)

from transformers import pipeline
sentiment_pipeline = pipeline("sentiment-analysis")

# https://huggingface.co/models?pipeline_tag=text-classification&sort=downloads&search=sentiment

specific_model = pipeline(model="finiteautomata/bertweet-base-sentiment-analysis")

!pip install accelerate -U

df.points.describe()

from sklearn.model_selection import train_test_split
import numpy as np

reviews = df.description_cleaned_tokenized
tuned_target = np.where(df.points.values > 92, 'pos', np.where(df.points.values < 85, 'neg', 'neu'))

X, y = reviews, tuned_target

# Use train_test_split to split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from transformers import TrainingArguments, Trainer

repo_name = "finetuning-sentiment-model-3000-samples"

training_args = TrainingArguments(
   output_dir=repo_name,
   learning_rate=2e-5,
   per_device_train_batch_size=16,
   per_device_eval_batch_size=16,
   num_train_epochs=2,
   weight_decay=0.01,
   save_strategy="epoch",
   push_to_hub=True,
)

trainer = Trainer(
   model=specific_model,
   args=training_args,
   train_dataset=X_train,
   eval_dataset=X_test,
  #  tokenizer=tokenizer,
  #  data_collator=data_collator,
  #  compute_metrics=compute_metrics,
)

from sklearn.feature_extraction.text import TfidfVectorizer

# Define a TfidVectorizer object. Remove all the uninformative words such as 'and,' 'the,' and 'him' from analysis. Bigrams only (ngram_range=(2,2)).
tfidf=TfidfVectorizer(stop_words="english", ngram_range=(2,2))

# Count the words in each description, calculate idf, and multiply idf by tf.
tfidf_matrix=tfidf.fit_transform(variety_description_2["description"])

# Resulting matrix should be # of descriptions (row) x # of bigrams (column)
tfidf_matrix.shape

from sklearn.metrics.pairwise import linear_kernel

# Compute the cosine similarity
cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)

variety_description_2=variety_description_2.reset_index()
indices = pd.Series(variety_description_2.index, index=variety_description_2['variety'])

def rec_from_grape(grape, cosine_sim=cosine_sim):
    # Get the index of the input wine
    idx = indices[grape]

    # Get the pairwise similarity scores between the input wine and all the wines
    sim_scores = list(enumerate(cosine_sim[idx]))

    # Sort the wines based on the similarity scores
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

    # Select the top three similarity scores
    sim_scores = sim_scores[1:4]

    # Get the grape variety indices
    wine_idx_list = [i[0] for i in sim_scores]

    # Create the output dataframe
    df=pd.DataFrame(columns=["similar wines", "Top 6 common words in wine reviews"])

    for wine_idx in wine_idx_list:

        g_variety=variety_description_2.iloc[wine_idx]["variety"]

        # Get top 6 common words in the review
        des=variety_description_2.iloc[wine_idx]["description"]

        if g_variety in variety_multi_reviews:     # If the wine has more than one reviews
            des_split=des.split(", ")
            key_words_list=des_split[:6]
            key_words_str=", ".join(key_words_list)

        else:
            key_words_str = des

        new_row={"similar wines": g_variety, "Top 6 common words in wine reviews": key_words_str}
        df=df.append(new_row, ignore_index=True)

    df.set_index("similar wines")

    # Widen the column width so that all common words could be displayed
    pd.set_option('max_colwidth', 500)

    return df

specific_model(df.description_cleaned_tokenized[0])[0]

# list(df.description_cleaned)

specific_model(list(df.description_cleaned)[:20])

df_test_mini = df[:20]
df_test_mini['test'] = specific_model(list(df.description_cleaned)[:20])

# Define a function to limit the number of words in a list
def limit_words(word_list, max_words=128):
    return word_list[:max_words]

# Apply the function to the DataFrame column
df_test_mini['word_list_column'] = df_test_mini['description_cleaned'].apply(lambda x: limit_words(x, max_words=128))

# df_test_mini.head()

# df_test_mini['sentiment_label'] = df_test_mini.test.label
# df_test_mini['sentiment_score'] = df_test_mini.test.score
df_test_mini.head()

df['description_cleaned_limit'] = df['description_cleaned'].apply(lambda x: limit_words(x, max_words=128))

df['description_cleaned_limit'].head()

df['base_sentiment_label'] = specific_model(list(df.description_cleaned_limit))

# df['base_sentiment_label'] = specific_model(df.description_cleaned_tokenized[0])

df['base_sentiment_label'].head()

df.to_csv('sentiment_data.csv')

import os
print(os.getcwd())

from google.colab import files
files.download('sentiment_data.csv')

wine_df = df

wine_df.loc[:, 'wineId'] = wine_df.loc[:, 'title'].astype('category').cat.codes

wine_df.loc[:, ['description', 'wineId', 'title']].head()

train_wine, test_wine = train_test_split(wine_df, train_size=0.05)

train_wine.reset_index(drop=True, inplace=True)

print(f"Training on {len(train_wine)} samples.")

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel
from sklearn.model_selection import train_test_split

tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df = 0, stop_words='english')

tfidf_matrix = tf.fit_transform(train_wine['description'])

print(f"The term-frequency inverse document frequency matrix is {tfidf_matrix.shape[0]} by {tfidf_matrix.shape[1]}")

# First finding the cosine similarities for the tfidf matrix
cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)


# Next, appending the results to a dictionary of the similar items to each wine
results = {}
for idx, row in train_wine.iterrows():
    similar_indices = cosine_similarities[idx].argsort()[:100:-1]
    similar_items = [(cosine_similarities[idx][i], train_wine['wineId'][i]) for i in similar_indices]
    results[row['wineId']] = similar_items[1:]

def item(id):
    return train_wine.loc[train_wine['wineId'] == id]['title'].tolist()[0].split(' - ')[0]

def recommend(item_id, num):
    print('Recommending ' + str(num) + ' products similar to ' + item(item_id) + ' ...')
    print('-----')
    recs = results[item_id][:num]
    for rec in recs:
        print('Recommended: ' + item(rec[1]) + '(score: ' + f"{rec[0]:.2f}" + ')')

# itemId (wineId) is grabbed from the trainset of wines

itemId_ = train_wine.loc[:, 'wineId'].values[0]
itemName_ = train_wine.loc[train_wine['wineId'] == itemId_, 'title'].values[0]

print(f"Using itemId {itemId_} which is {itemName_} \n")

# The recommend function is then run to find and return the top num matches (5 in this case)

recommend(item_id=itemId_, num=5)



# We can then compare the descriptions of the two wines to see how they match

description_original = train_wine.loc[train_wine['title'] == itemName_, 'description'].values[0]

description_matched = train_wine.loc[train_wine['wineId'] == results[itemId_][0][1], 'description'].values[0]

print(f"First wine description: \n{description_original} \n\nMatched wine description: \n{description_matched}")

from sklearn.neighbors import NearestNeighbors
from scipy.sparse import csr_matrix
from sklearn.decomposition import TruncatedSVD

data_recommend = df[['province','variety','points']]
data_recommend.dropna(axis=0, inplace=True)
data_recommend.drop_duplicates(['province','variety'], inplace=True)

data_pivot = data_recommend.pivot(index= 'variety',columns='province',values='points').fillna(0)
data_matrix = csr_matrix(data_pivot)

knn = NearestNeighbors(n_neighbors=10, algorithm= 'brute', metric= 'cosine')
model_knn = knn.fit(data_matrix)

# sample 5
for n in range(5):
    query_index = np.random.choice(data_pivot.shape[0])
    #print(n, query_index)
    distance, indice = model_knn.kneighbors(data_pivot.iloc[query_index].values.reshape(1,-1), n_neighbors=6)
    for i in range(0, len(distance.flatten())):
        if  i == 0:
            print('Recmmendation for ## {0} ##:'.format(data_pivot.index[query_index]))
        else:
            print('{0}: {1} with distance: {2}'.format(i,data_pivot.index[indice.flatten()[i]],distance.flatten()[i]))
    print('\n')

#

from sklearn.neighbors import NearestNeighbors
from scipy.sparse import csr_matrix
from sklearn.decomposition import TruncatedSVD

wine1 = df.copy()

# Lets choice rating of wine is points, title as user_id, and variety,
col = ['province','variety','points']
wine1 = wine_df[col]
wine1 = wine1.dropna(axis=0)
wine1 = wine1.drop_duplicates(['province','variety'])
wine1 = wine1[wine1['points'] >85]
wine_pivot = wine1.pivot(index= 'variety',columns='province',values='points').fillna(0)
wine_pivot_matrix = csr_matrix(wine_pivot)

knn = NearestNeighbors(n_neighbors=10,algorithm= 'brute', metric= 'cosine')
model_knn = knn.fit(wine_pivot_matrix)

# Predict

query_index = np.random.choice(wine_pivot.shape[0])
distance, indice = model_knn.kneighbors(wine_pivot.iloc[query_index,:].values.reshape(1,-1),n_neighbors=6)
for i in range(0, len(distance.flatten())):
    if  i == 0:
        print('Recmmendation for {0}:\n'.format(wine_pivot.index[query_index]))
    else:
        print('{0}: {1} with distance: {2}'.format(i,wine_pivot.index[indice.flatten()[i]],distance.flatten()[i]))

# df['baseline_sentiment'] = None
# df['baseline_sentiment_label'] = None
# df['baseline_sentiment_score'] = None
# for i in range(len(df)):
#   try:
#     sent = specific_model(list(df.description_cleaned)[i])[0]
#     df['baseline_sentiment'][i] = sent
#     df['baseline_sentiment_label'][i] = sent.label
#     df['baseline_sentiment_score'][i] = sent.score
#   except Exception as e:
#     print('unable to run', e)
#     continue

# rf

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

numeric_columns

# Specify columns to exclude
columns_to_exclude = ['points'] #, 'id']

# Use loc to select all columns except the specified ones
# selected_columns = df.loc[:, ~df.columns.isin(columns_to_exclude)]
numeric_columns = df.select_dtypes(exclude=['object'])
selected_columns = numeric_columns.loc[:, ~numeric_columns.columns.isin(columns_to_exclude)]

X_train, X_test, y_train, y_test = train_test_split(selected_columns, df.sentiment_label, test_size=0.2, random_state=42)



# Initialize the Random Forest classifier
random_forest = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model on the training data
random_forest.fit(X_train.fillna(0), y_train)

# Make predictions on the testing data
y_pred = random_forest.predict(X_test.fillna(0))

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

# Print the evaluation metrics
print(f"Accuracy: {accuracy}")
print("Confusion Matrix:\n", conf_matrix)
print("Classification Report:\n", class_report)

len(df), len(list(df.description_cleaned))

from transformers import AutoModelForSequenceClassification, AutoConfig
from transformers import TFAutoModelForSequenceClassification
from transformers import AutoTokenizer
import numpy
import numpy as np
from scipy.special import softmax
import tensorflow as tf

MODEL = f"cardiffnlp/twitter-roberta-base-sentiment-latest"
tokenizer = AutoTokenizer.from_pretrained(MODEL)
config = AutoConfig.from_pretrained(MODEL)
model = AutoModelForSequenceClassification.from_pretrained(MODEL)


#distilbert

model_name_d="distilbert-base-uncased-finetuned-sst-2-english"

tokenizer = AutoTokenizer.from_pretrained(model_name_d)

config = AutoConfig.from_pretrained(model_name_d)

model = AutoModelForSequenceClassification.from_pretrained(model_name_d)

#bert

from transformers import BertTokenizer, BertForSequenceClassification,BertConfig

model_name_b='bert-base-uncased'

tokenizer = BertTokenizer.from_pretrained(model_name_b)

config = BertConfig.from_pretrained(model_name_b)

model = BertForSequenceClassification.from_pretrained(model_name_b)

def sentiment_labels(text):
    encoded_input = tokenizer(text, padding=True,truncation=True,max_length=512, return_tensors='pt')
    output = model(**encoded_input)
    scores = output[0][0].detach().numpy()
    scores = softmax(scores)
    ranking = np.argsort(scores)
    ranking = ranking[::-1]
    return config.id2label[ranking[0]]

text=" Very Good Service offered by Team."
sentiment_labels(text)

# fine tuned sentiments

df = pd.read_csv(f"{MAIN_DIR}/sentiment_data.csv")
df.head()

import ast

df['base_sentiment_label_dict'] = df['base_sentiment_label'].apply(lambda x: ast.literal_eval(x.strip()))

df['base_sentiment_label_dict'].head()

df[['sentiment', 'sentiment_score']] = df['base_sentiment_label_dict'].apply(lambda x: pd.Series([x['label'], x['score']]))

import matplotlib.pyplot as plt
sentiment_counts = df.groupby(['sentiment']).size()
print(sentiment_counts)

# Let's visualize the sentiments
fig = plt.figure(figsize=(6,6), dpi=100)
ax = plt.subplot(111)
sentiment_counts.plot.pie(ax=ax, autopct='%1.1f%%', startangle=270, fontsize=12, label="")

"""## Inference"""

# Specify columns to exclude
columns_to_exclude = ['points'] #, 'id']

# Use loc to select all columns except the specified ones
# selected_columns = df.loc[:, ~df.columns.isin(columns_to_exclude)]
numeric_columns = df.select_dtypes(exclude=['object'])
selected_columns = numeric_columns.loc[:, ~numeric_columns.columns.isin(columns_to_exclude)]

X_train, X_test, y_train, y_test = train_test_split(selected_columns, df.sentiment, test_size=0.2, random_state=42)



# Initialize the Random Forest classifier
random_forest = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model on the training data
random_forest.fit(X_train.fillna(0), y_train)

# Make predictions on the testing data
y_pred = random_forest.predict(X_test.fillna(0))

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

# Print the evaluation metrics
print(f"Accuracy: {accuracy}")
print("Confusion Matrix:\n", conf_matrix)
print("Classification Report:\n", class_report)

# Create a single DataFrame that contains variety and description only. Delete any rows that are duplicated or contain missing data.
variety_description= df[["variety", "description"]]
variety_description=variety_description.drop_duplicates().dropna()
variety_description.head()

len(variety_description["variety"].unique().tolist())

# Create and display the chart showing the number of reviews per grape variety for the top 30 wines
variety_description["variety"].value_counts().iloc[:30].plot.bar()

# Count the number of reviews per grape variety. This returns a series.
variety_rev_number=variety_description["variety"].value_counts()

# Convert the Series to Dataframe
df_rev_number=pd.DataFrame({'variety':variety_rev_number.index, 'rev_number':variety_rev_number.values})
df_rev_number[(df_rev_number["rev_number"]>1)].shape

# Create a ist of grape varieties that have more than one review
variety_multi_reviews=df_rev_number[(df_rev_number["rev_number"]>1)]["variety"].tolist()

# Create a ist of grape varieties that have only one review
variety_one_review=df_rev_number[(df_rev_number["rev_number"]==1)]["variety"].tolist()

# Set index once
# variety_description_2 = variety_description.set_index("variety")
# variety_description = variety_description.set_index("variety")

from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer



# Define a CountVectorizer object
    # stop_words="english": Remove all the uninformative words such as 'and', 'the' from analysis
    # ngram=range(1,2): means unigrams and bigrams
cv=CountVectorizer(stop_words="english", ngram_range=(2,2))

# Define a TfidfTransformer object
tfidf_transformer=TfidfTransformer(smooth_idf=True, use_idf=True)

for grape in variety_multi_reviews:

    df=variety_description.loc[[grape]]

    # Generate word counts for the words used in the reviews of a specific grape variety
    word_count_vector=cv.fit_transform(df["description"])

    # Compute the IDF values
    tfidf_transformer.fit(word_count_vector)

    # Obtain top 100 common words (meaning low IDF values) used in the reviews. Put the IDF values in a DataFrame
    df_idf=pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names_out(), columns=["idf_weights"])
    df_idf.sort_values(by=["idf_weights"], inplace=True)

    # Collect top 100 common words in a list
    common_words=df_idf.iloc[:100].index.tolist()

    # Convert the list to a string and create a dataframe
    common_words_str=", ".join(elem for elem in common_words)
    new_row= {"variety":grape, "description":common_words_str}

    # Add the variety and its common review words to a new dataframe
    variety_description_2=variety_description_2.append(new_row, ignore_index=True)

from transformers import pipeline
classifier = pipeline(
                      task="zero-shot-classification",
#                       device=0,
                      model="facebook/bart-large-mnli"
                    )
text = 'I do not like this recommendation'
classifier(text,["positive","negative",'neutral'],multi_class=True)